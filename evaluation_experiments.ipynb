{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments\n",
    "\n",
    "This notebook demonstrates the experiments carried out for \"Evaluating Explainability of Graph Neural Networks for Disease Subnetwork Detection\".\n",
    "\n",
    "The code here runs multiple iterations of the process: training a model -> running the explainer -> generating explanations -> evaluating explanability metrics.\n",
    "\n",
    "Four explainability metrics are used: validity+, validity-, sparsity and fidelity. The average of each metric in each iteration is reported and saved to file at the end of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "First, we set up the necessary imports and the variables that will be used later:\n",
    "\n",
    "- Number of times to run the entire workflow\n",
    "```\n",
    "big_loop_iterations = 10 (recommended)\n",
    "```\n",
    "- Number of times to run the explainer on a trained model before evaluating it (the mean explanation is used for the evaluation)\n",
    "```\n",
    "explainer_runs = 10 (recommended)\n",
    "```\n",
    "- Array of thresholds used for transforming the soft node mask to a hard node mask. Validity+ and validity- are calculated at every threshold in the given array . \n",
    "\n",
    "Optionally, fidelity is calculated at every threshold on the given array (see use_softmask_fidelity).\n",
    "\n",
    "The number here is the percentage of nodes that are taken as important. For example, at threshold 30, the top 30% of values are selected as important. \n",
    "```\n",
    "threshold = [30, 50]\n",
    "```\n",
    "- The number of samples to be used when evaluating fidelity.\n",
    "```\n",
    "samples = [5, 10, 20]\n",
    "```\n",
    "- Whether or not to use a soft mask to evaluate fidelity. If not using a soft mask, hard masks at each of the thresholds are used to evaluate fidelity.\n",
    "```\n",
    "use_softmask_fidelity = False\n",
    "```\n",
    "- The filepath (any signifier you choose) used to name the directory and files where the results are saved.\n",
    "```\n",
    "filepath = \"23_May_KIRC_data\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See description above for what these variables represent\n",
    "big_loop_iterations = 1\n",
    "explainer_runs = 1\n",
    "thresholds = [30, 50]\n",
    "# thresholds = [90, 80, 70, 60, 50, 40, 30, 20, 10]\n",
    "samples = 10\n",
    "use_softmask_fidelity = False\n",
    "\n",
    "# Used to label the resulting files - \n",
    "filepath = \"6 Jun\"\n",
    "\n",
    "# Set up directory for result files\n",
    "import os\n",
    "dir = f'./results_{filepath}'\n",
    "if not os.path.exists(dir):\n",
    "    os.mkdir(dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start, we choose a dataset to work with (either KIRC or synthetic data) and load this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from GNNSubNet import GNNSubNet as gnn\n",
    "from GNNSubNet import explainability_evaluator as eval\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# # Kidney data set  ------------------------- #\n",
    "# loc   = \"GNNSubNet/datasets/kirc/\"\n",
    "# ppi   = f'{loc}/KIDNEY_RANDOM_PPI.txt'\n",
    "# feats = [f'{loc}/KIDNEY_RANDOM_Methy_FEATURES.txt', f'{loc}/KIDNEY_RANDOM_mRNA_FEATURES.txt']\n",
    "# targ  = f'{loc}/KIDNEY_RANDOM_TARGET.txt'\n",
    "\n",
    "# # Synthetic data set  ------------------------- #\n",
    "loc   = \"GNNSubNet/datasets/synthetic/\"\n",
    "ppi   = f'{loc}/NETWORK_synthetic.txt'\n",
    "feats = [f'{loc}/FEATURES_synthetic.txt']\n",
    "targ  = f'{loc}/TARGET_synthetic.txt'\n",
    "\n",
    "# Read in the synthetic data\n",
    "g = gnn.GNNSubNet(loc, ppi, feats, targ, normalize=False)\n",
    "\n",
    "# Get some general information about the data dimension\n",
    "g.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run\n",
    "\n",
    "Run the workflow of train, explain and evaluate for the given number of big_loop_iterations.\n",
    "\n",
    "NB: if running on KIRC data for 10 iterations with all metrics, this cell can take **5 to 7 hours** to fully run!\n",
    "\n",
    "NB: this cell only saves the raw results for each test graph in each iteration. The cell below gathers the averages of all metrics into a single results table and saves the results table. **Ensure that both cells are run together to obtain the full results.**\n",
    "\n",
    "Read the description further below to see the format in which these results are saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_info = []\n",
    "fidelity = []\n",
    "validity_plus = []\n",
    "validity_minus = []\n",
    "validity_plus_matrix = []\n",
    "validity_minus_matrix = []\n",
    "sparsity = []\n",
    "\n",
    "for i in range(big_loop_iterations):\n",
    "    g = gnn.GNNSubNet(loc, ppi, feats, targ, normalize=False)\n",
    "    g.train()\n",
    "\n",
    "    # Check the performance of the classifier\n",
    "    accuracy = g.accuracy\n",
    "\n",
    "    # Run the explainer the desired number of times\n",
    "    g.explain(explainer_runs)\n",
    "\n",
    "    # Save node mask\n",
    "    np.savetxt(f\"results_{filepath}/{i}_node_mask.csv\", g.node_mask, delimiter=\",\", fmt= \"% s\")\n",
    "\n",
    "    # Initialise evaluator\n",
    "    ev = eval.explainability_evaluator(g)\n",
    "\n",
    "    if use_softmask_fidelity:\n",
    "        # Fidelity with softmask\n",
    "        f = ev.evaluate_RDT_fidelity(use_softmask=True, samples = samples)\n",
    "\n",
    "        # Save raw results in case needed for further analysis\n",
    "        filename = f\"results_{filepath}/{i}_fidelities_{samples}_samples_softmask.csv\"\n",
    "        np.savetxt(filename, f, delimiter=',', fmt ='% s')\n",
    "\n",
    "        # Save mean fidelity to list to create processed table\n",
    "        fidelity.append( [i, accuracy, samples, 0, np.mean(f)])\n",
    "    else:\n",
    "        # Fidelity with hardmasks of varying thresholds\n",
    "        for t in thresholds:\n",
    "            f = ev.evaluate_RDT_fidelity(use_softmask=False, samples = samples, threshold=t)\n",
    "\n",
    "            # Save raw results in case needed for further analysis\n",
    "            filename = f\"results_{filepath}/{i}_fidelities_{samples}_samples_top_{t}_hardmask.csv\"\n",
    "            np.savetxt(filename, f, delimiter=',', fmt ='% s')\n",
    "\n",
    "            # Save mean fidelity to list to create processed table\n",
    "            fidelity.append( [i, accuracy, samples, t, np.mean(f)])\n",
    "\n",
    "    # Sparsity\n",
    "    sparsities = ev.evaluate_sparsity()\n",
    "    # Save raw results in case needed for further analysis\n",
    "    filename = f\"results_{filepath}/{i}_sparsities.csv\"\n",
    "    np.savetxt(filename, sparsities, delimiter=',', fmt ='% s')\n",
    "    # Save mean sparsity to list to create processed table\n",
    "    sparsity.append([i, accuracy, np.mean(sparsities)])\n",
    "\n",
    "    # Validity with hardmasks of varying thresholds\n",
    "    for t in thresholds:\n",
    "        v_plus, v_minus, mat_plus, mat_minus = ev.evaluate_validity(threshold=t, confusion_matrix=True)\n",
    "        validity_plus.append([i, accuracy, t, v_plus])\n",
    "        validity_minus.append([i, accuracy, t, v_minus])\n",
    "        validity_plus_matrix.append([i, accuracy, t, mat_plus[0,0], mat_plus[0,1], mat_plus[1,0], mat_plus[1,1]])\n",
    "        validity_minus_matrix.append([i, accuracy, t, mat_minus[0,0], mat_minus[0,1], mat_minus[1,0], mat_minus[1,1]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "Finally, we can view the processed tables and save them to a written file.\n",
    "\n",
    "Four CSV files are created, one for each metric. You can see the columns that each of them contains in the code snippet below.\n",
    "\n",
    "Additionally, a single CSV file named \"results_{filepath}.csv\" contains the combined results of all four metrics.\n",
    "\n",
    "All files are placed in a directory named \"results_{filepath}\".\n",
    "\n",
    "**NB: See visualisation_and_analysis.ipynb for the final results and plots.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Iteration  Model accuracy  Sparsity score\n",
      "0          0           100.0        0.087605\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Iteration</th>\n",
       "      <th>Model accuracy</th>\n",
       "      <th>Samples</th>\n",
       "      <th>Threshold</th>\n",
       "      <th>Fidelity score</th>\n",
       "      <th>Validity+ score</th>\n",
       "      <th>Validity- score</th>\n",
       "      <th>Sparsity score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>10</td>\n",
       "      <td>30</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.53</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.087605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>10</td>\n",
       "      <td>50</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.55</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.087605</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Iteration  Model accuracy  Samples  Threshold  Fidelity score  \\\n",
       "0          0           100.0       10         30             1.0   \n",
       "1          0           100.0       10         50             1.0   \n",
       "\n",
       "   Validity+ score  Validity- score  Sparsity score  \n",
       "0             0.53              1.0        0.087605  \n",
       "1             0.55              1.0        0.087605  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "fidelity_table = pd.DataFrame(fidelity, columns=['Iteration','Model accuracy','Samples','Threshold','Fidelity score'])\n",
    "fidelity_table.to_csv(f\"results_{filepath}/fidelity_table.csv\", float_format=\"%.3f\", index=False)\n",
    "\n",
    "sparsity_table = pd.DataFrame(sparsity, columns=[\"Iteration\", \"Model accuracy\", \"Sparsity score\"])\n",
    "print(sparsity_table)\n",
    "sparsity_table.to_csv(f\"results_{filepath}/sparsity_table.csv\", float_format=\"%.3f\", index=False)\n",
    "\n",
    "validity_table_plus = pd.DataFrame(validity_plus, columns=[\"Iteration\", \"Model accuracy\", \"Threshold\", \"Validity+ score\"])\n",
    "validity_table_plus.to_csv(f\"results_{filepath}/validity_table_plus.csv\", float_format=\"%.3f\", index=False)\n",
    "\n",
    "validity_table_minus = pd.DataFrame(validity_minus, columns=[\"Iteration\", \"Model accuracy\", \"Threshold\", \"Validity- score\"])\n",
    "validity_table_minus.to_csv(f\"results_{filepath}/validity_table_minus.csv\", float_format=\"%.3f\", index=False)\n",
    "\n",
    "validity_plus_matrix = pd.DataFrame(validity_plus_matrix, columns=[\"Iteration\", \"Model accuracy\", \"Threshold\", \"00\", \"01\", \"10\", \"11\"])\n",
    "validity_plus_matrix.to_csv(f\"results_{filepath}/validity_matrix_plus.csv\", index=False)\n",
    "\n",
    "validity_minus_matrix = pd.DataFrame(validity_minus_matrix, columns=[\"Iteration\", \"Model accuracy\", \"Threshold\", \"00\", \"01\", \"10\", \"11\"])\n",
    "validity_minus_matrix.to_csv(f\"results_{filepath}/validity_matrix_minus.csv\", index=False)\n",
    "\n",
    "# Construct a single table holding all the metrics\n",
    "final_table = fidelity_table\n",
    "final_table[\"Validity+ score\"] = validity_table_plus[\"Validity+ score\"]\n",
    "final_table[\"Validity- score\"] = validity_table_minus[\"Validity- score\"]\n",
    "final_table = pd.merge(final_table, sparsity_table, on=[\"Iteration\", \"Model accuracy\"], how='outer')\n",
    "final_table.to_csv(f\"results_{filepath}/results_{filepath}.csv\", index=False)\n",
    "\n",
    "final_table"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnnenv_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
